论文出发点在于，克服点监督中因为标签稀疏带来的局限性，使模型能够以更鲁棒、更完整的方式学习时序动作的边界。与该领域的其他方法相似，论文的框架同样接收点级标签作为输入。但与它们相比，论文目标是提供一个系统性的框架，该框架将这些稀疏的点标签作为“种子”，用以在整个视频中发现并建模那些信息量最大、最能代表完整动作的“代表性片段”。
需要注意的是，论文没有在提出一种全新的特征提取主干网络，而是提出一个创新的框架，其核心在于各个组件之间的协同作用——即动作补偿采样、特征重分配和代表性片段选取这三个模块，是如何结合在一起去共同完成从稀疏到密集的信号转化，并最终实现精准定位的。
① 动作补偿采样
算法核心目的非常直接：为了解决短时动作因信号微弱而容易被模型忽略的问题，通过上采样技术，在特征层面人为地增强短动作的表达，从而降低漏检率。
整个算法的实现流程可以分为三个步骤：识别短动作 -> 计算采样权重 -> 基于权重进行重采样。
第一步：识别短时动作
算法首先需要知道“对谁”进行补偿。这个过程依赖于模型的一轮初步预测。
1. 模型运行一次，生成一系列动作提案，每个提案 j 都包含一个预测的开始时间 s_j 和结束时间 e_j。
2. 设定一个固定的时间阈值\ \tau。
3. 遍历所有提案，如果某个提案的时长 \left(e_j-s_j\right) 小于 \tau，则该提案所覆盖的时间区域就被标记为需要补偿的“短时动作区域”。

第二步：计算采样权重 
识别出目标区域后，算法需要为视频的每一帧计算一个采样权重，以决定后续重采样的密度。
1. 初始化一个与视频原始特征序列等长的权重向量 W，所有元素的初始值均为1。这意味着默认情况下，所有区域的采样密度相同。
2. 对于上一步识别出的每一个短时动作区域（从 s_j到 e_j），算法使用以下公式更新该区域内所有帧的权重值：W\left[s_j:e_j\right]=\frac{\tau}{e_j-s_j}
这个公式确保了动作越短，其获得的权重加成越大。例如，一个时长为 \tau/2的动作，其权重会被更新为2；而一个更短的、时长为 \tau/4 的动作，其权重会被更新为4。最终，得到一个权重“地图”，其中短动作区域的权重值大于1，其余区域的权重值为1。

第三步：逆变换采样与特征
这是执行“补偿”操作的核心步骤。算法将利用刚刚计算出的权重向量 W 来生成一套全新的、更长的特征序列和标签序列。
1. 生成新采样点：算法采用逆变换采样方法。简单来说，它将权重向量 W 视为一个概率密度分布，权重越高的区域，被采样的概率就越大。通过这个加权采样过程，算法会生成一个新的、更长的、分布不均匀的采样点序列。
2. 生成新特征：有了新的采样点序列，但论文还没有对应的特征。算法通过线性插值方法，基于原始的特征序列 X，为这些新的采样点计算出对应的特征值，从而构建出最终的、更长的特征序列 X_{up}。
3. 更新标签位置：由于特征序列的长度和结构都发生了变化，原始的点标签位置也必须进行同步更新，它根据采样点位置的新旧映射关系，为每一个原始的点标签计算出它在新的序列 X_{up}中应该处于的新时间戳。
最终输出：一套全新的、更长的特征序列，以及一套与之位置精确对齐的、更新过的点标签。

② 特征重分配
这个算法的核心目的可以概括为“特征提纯”。经过补偿采样后，有了一套更长的特征序列，但这些特征动作和背景的特征在某些区域可能界限不清。特征重分配对每一帧的特征进行优化，让属于动作的特征更具“动作”的共性，属于背景的特征更具“背景”的共性，从而增强特征的整区分度。
该算法的实现借鉴了期望最大化（EM）注意力机制的思想。 
第一步：初始化 K 个“特征原型”
算法首先会随机初始化 K 个“特征原型”（Prototypes）。在数学上，这是一个权重矩阵 \mu，其维度为 K\ \times D，其中 K 是原型的数量，D 是特征的维度。这些原型将作为标准，用来衡量视频中的每一帧特征更偏向于特征还是背景。

第二步：计算相似度
有了视频的特征序列 F（维度为 T × D）和 K 个特征原型 \mu。论文需要计算出每一帧的特征，分别与这 K 个原型的 相似度。
Z=\mathrm{softmax}\left(\gamma\cdot\mathrm{Norm}\left(F\right)\cdot\mathrm{Norm} \left(\mu\right)^T\right)
	Norm() 代表L2归一化，用于消除长度的影响，只关注方向。
	F\cdot\mu^T 的矩阵乘法，计算了每一帧的特征与每一个原型的点积相似度。
	softmax 函数将这些原始的相似度分数，转换为概率分布。计算完成后，Z_{tk} 就代表了第 t 帧的特征“属于”第 k 个原型的概率。
	 \gamma 是一个超参数，用于控制这个概率分布的平滑度。

第三步：更新“特征原型”
有了归属关系 Z 之后，就可以反过来优化 “特征原型” \mu 了，让它们更能代表视频中的真实特征分布。
更新的逻辑是：一个新的、更优的原型 \mu^\ast，应该是所有视频帧特征 F 的加权平均，而权重就是这些帧对该原型的“亲和度” Z。为了防止更新步子迈得太大，这里引入了一个权衡参数 \omega：
\mu^\ast=\omega\cdot\mathrm{Norm}\left(Z\right)^T\cdot F+\left(1-\omega\right)
	\mathrm{Norm}\left(Z\right)^T\cdot F 就是上述的加权平均过程。
	 \left(1-\omega\right)则保留了一部分旧原型的信息，起到了平滑更新的作用。
	 \omega 控制了新旧信息融合的比例。

第四步：生成重分配特征
最后一步，利用更新的特征原型 \mu^\ast，来重新构建每一帧的原始特征，得到最终的输出 F'。
其逻辑与第三步类似：每一帧的新特征 F_t^\prime，应该是所有新原型\mu^\ast 的加权平均，而权重就是该帧对各个原型的“亲和度” Z。同样，这里也用 \omega来平衡新生成的特征与原始特征：
F^\prime=\omega\cdot Z\cdot\mu^\ast+\left(1-\omega\right)F
这个公式的直观理解是：每一帧的新特征，都是由它所属的那些“概念原型”的特征，根据其归属概率进行混合，再加上一部分它自身的原始特征共同构成的。
经过这个算法，输出的特征序列 F' 相比于输入 F，动作与背景能够更好地分离。

③ 代表性片段选取
现在论文已经有了较高质量的特征，要利用去找到并监督一个完整的动作区间。“代表性片段选取”就是设计一套有效的评分机制，来自动评估视频中成千上万个候选片段的“质量”，并找出那些最有可能完整包含了一个动作实例的“代表性片段”。然后，通过专门设计的损失函数，引导模型去优化这些代表性片段的预测，从而实现对动作边界的精确学习。
这个模块的核心是 “内外对比”策略。整个流程可以分解为三个步骤：生成伪标签、计算内外对比分数、构建损失函数。
第一步：生成密集的伪标签
模型会利用现有的点标签和初步预测，生成一套伪点标签序列。这些伪标签虽然不完美，但它们将稀疏的点信号扩展到了整个视频，为后续的评分提供了基础。

第二步：计算内外对比分数
这是精髓所在。对于视频中某个特定动作类别 c，算法会生成大量候选的片段 F_c。对于其中的任意一个片段（我称之为第 m 个片段，时间范围从s_m 到 e_m），算法会计算一个代表性分数 R\left(F_c^m\right)。
这个分数的计算方法，就是“内外对比”：
1. 计算内部得分：获取该片段内部（从 s_m 到 e_m）所有帧的平均动作置信度。这个得分越高，说明片段内部越像一个动作。
2. 计算外部得分：获取该片段紧邻的外部区域所有帧的平均动作置信度。这个得分越低，说明片段的外部区域越像背景。
3. 计算对比分数：代表性分数 R\left(F_c^m\right) 就是由内部得分和外部得分通过函数计算得出的。
这意味着，内外得分的差距越大，这个片段的边界就越清晰，就越有代表性。

第三步：构建代表性损失
1. 选取最佳片段：对于视频中的每一个动作类别，算法会根据刚刚计算的内外对比分数 R\left(F_c\right)，挑选出得分最高的那个片段，称之为最佳代表性片段 F_c^{best}。
2. 定义优化目标：对于这个最佳片段 F_c^{best}，优化目标是：
	内部的动作分数应该最大化（趋近于1）。
	外部的动作分数应该最小化（趋近于0）。
	综合来看，它的内外对比分数 R\left(F_c^{best}\right) 应该最大化。
3. 构建分数对比损失：为了实现这个目标，论文设计了一个简单的平方损失函数：

L_{score}=\frac{1}{\sum y^{vid}}\sum_{c=1}^{C}{y^{vid}\left[c\right]}\cdot\left(1-R\left(F_c^{best}\right)\right)^2

这个公式的含义是：对于视频中真实存在的每一个动作类别 c (由 y^{vid}\left[c\right] 判断)，论文要惩罚那些使得最佳代表性片段的内外对比分数 R\left(F_c^{best}\right) 偏离理想值1的情况。通过最小化这个损失，模型就被迫去调整其预测，以产生内外对比更强烈的、边界更清晰的最佳片段。
4. 构建特征对比损失：此外，论文还引入了一个辅助的特征对比损失 L_{feat}，其思想是：来自不同动作实例但属于同一类别的代表性特征，应该在特征空间中相互靠近。这进一步增强了模型的类内聚性。
最终的总代表性损失 L_{pres} 是这两个损失的和。

④ 联合训练和多步优化
现在将前面提到的所有算法以及基础的分类/定位任务，通过一个统一的损失函数结合起来；在此基础上，设计一种迭代式的训练流程，让模型的性能在训练过程中不断地自我完善。
第一步：构建联合损失函数 
模型的最终训练目标，是由四个不同层面的损失函数加权求和构成的总损失 L_{total}：
L_{total}=\lambda_1L_{vid}+\lambda_2L_{point}+\lambda_3L_{pres}+\lambda_4L_{ins}
这四个部分分别是：
	L_{vid} (视频级损失): 基础的视频分类损失，确保模型能正确识别视频中包含哪些动作类别。
	L_{point} (点级损失): 针对原始点标签位置的分类损失，确保在标注点位置有较高的动作置信度。
	L_{pres} (代表性损失): 论文刚刚详述的核心损失，用于驱动模型学习精确的动作边界。
	L_{ins} (伪实例损失): 基于模型自己生成的密集伪标签进行的辅助损失，提供额外的密集监督信号。
通过最小化这个联合损失，模型被要求视频级、代表性片段、微观点级三个层面都做出准确的预测。

第二步：多步提案优化
1. 第一轮训练：模型从零开始进行标准训练，生成一套初步的、比较粗糙的动作提案 S_1。
2. 进入循环 (第二轮及以后)：
	反馈：上一轮生成的动作提案 S_{z-1} 被“反馈”给动作补偿采样模块，从而进行更精准的补偿采样，生成更高质量的特征 F_z。
	再训练：模型利用这套更高质量的特征 F_z 和更新过的伪标签，进行新一轮的训练，从而得到一套更精准的动作提案 S_z。
3. 重复：这个预测-反馈-再训的过程重复多次。
